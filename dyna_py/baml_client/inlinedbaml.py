# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n\nclient<llm> CustomO3Mini {\n  provider openai\n  options {\n    model \"o3-mini\"\n    api_key env.OPENAI_API_KEY\n    temparature 1.0\n  }\n}\n\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n    temperature 1.0\n  }\n}\n\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomRoundRobin {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomO3Mini, CustomGPT4o, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4o]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    mutliplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.205.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n\n",
    "jockular.baml": "\n\nfunction JokeTeller(arg:string) -> string {\n    client CustomGPT4oMini    \n    prompt #\"\n\n        {{ctx.output_format}}\n\n        {{arg}}\n        \n    \"#\n}\n\n\nclass MonikerState {\n    topic string?          // generic focus of the task (formerly \"subject\"); optional for broad tasks\n    last_style string?     // last style/tone used (generic)\n    last_output string?    // last emitted human-facing text (generic)\n}\n\nclass MonikerGuidance {\n    topic string? @description(#\"\n        optional one-shot override for topic\n    \"#)         \n    style string?  @description(#\"\n        optional style/tone hint (e.g., \"dry\", \"formal\", \"playful\")\n    \"#)         \n}\n\nclass MonikerData {\n    topic string? @description(#\"\n     simple structured echo for consumers; extend as needed\n        \n    \"#)         \n}\n\nclass MonikerStepFrameIn {\n    step string \n    state MonikerState\n    guidance MonikerGuidance?\n}\n\nclass MonikerStepFrameOut {\n    step string\n    state MonikerState\n    next_step string\n    \n    text string @description(#\"\n        human-facing output for this step\n    \"#)           \n    data MonikerData @description(#\"\n        structured output (minimal, extendable)\n    \"#)      \n    done bool @description(#\"\n        true when the monikerâ€™s workflow is complete\n    \"#)             \n    notes string?  @description(#\"\n         short rationale/what changed\n    \"#)        \n}\n\n// Generic moniker runner: provide the task description + a StepFrame input.\n// Examples of task: \n//  - \"Tell a short witty joke about the topic.\"\n//  - \"Write a two-sentence summary of the topic.\"\n//  - \"List three key facts about the topic.\"\n\n\nfunction TellAJoke(frame: MonikerStepFrameIn, task: string)-> MonikerStepFrameOut  {\n    client CustomGPT4oMini\n    prompt #\"\n         You are a moniker that performs exactly one step of work according to 'task'.\n\n        {{ctx.output_format}}\n\n        Requirements:\n        - Perform the task based on state and (if present) guidance.\n        - If guidance.topic is present, use it and update state.topic.\n        - If guidance.style is present, reflect it in the output and set state.last_style.\n        - Place the human-facing output in 'text'.\n        - Set data.topic to the final topic used for this step.\n        - Set state.last_output to the same 'text' value.\n        - Echo back the input 'step' as 'step'.\n        - set next_step to be the next step as indicated.\n        - Set 'done' true only if the task is inherently single-shot or indicates completion; otherwise false.\n        - Do not add extra keys or commentary.\n        \n        # step 0:\n        - keep telling jokes\n        # step 1\n        - tell bigger jokes\n        - next_step 2.1\n        # step 2.1\n        - tell sad joke\n        - next_step 2.2\n        # step 2.2\n        - tell sad joke\n        - next_step 3\n        \n        # step 3\n        - ok we are done\n\n        -- TASK --\n        {{ task }}\n        -- END TASK --\n\n        -- INPUT FRAME --\n        {{ frame }}\n        -- END INPUT FRAME --\n    \"#\n}",
}

def get_baml_files():
    return _file_map
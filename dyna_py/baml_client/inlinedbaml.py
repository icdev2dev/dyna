# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n\nclient<llm> CustomO3Mini {\n  provider openai\n  options {\n    model \"o3-mini\"\n    api_key env.OPENAI_API_KEY\n    temparature 1.0\n  }\n}\n\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n    temperature 1.0\n  }\n}\n\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomRoundRobin {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomO3Mini, CustomGPT4o, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4o]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    mutliplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.205.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n\n",
    "jockular.baml": "enum GenericStepOutStatus {\n    OK \n    ERR   \n}\nenum GenericStepOutControl {\n    CONTINUE\n    PAUSE\n    STOP \n    DONE\n}\n\nclass StepFrameIn {\n    context string @description(#\"\n        This is an json encoded string input field to provide exact context of the task at hand. \n    \"#)\n    guidance string @description(#\"\n        This is an json encoded string input field and it will provide you additional guidance on a mutiple step workflow if required. \n    \"#)\n}\n\nclass StepFrameOut {\n        \n    status GenericStepOutStatus? @description(#\"\n        This is the status of the request. Generally ok.\n    \"#)\n    control GenericStepOutControl? @description(#\"\n        This is to give you some agency to indicate to an outside loop that is \n        controlling your excecution on whether to continue, pause, stop or be done.\n\n        Most of time your response would be to continue. \n        If anything else than continue, you will need to provide a reason\n        If it is pause, you would also populate hints to indicate when to resume        \n    \"#)\n    reason string? @description(#\"\n        if the decision is anything other than continue, then give reason.\n    \"#)\n    hint string? @description(#\"\n        if the control is pause, then give some hint when to resume.\n    \"#)\n\n    text string? @description(#\"\n        this is the human readable output \n    \"#)\n    payload string? @description(#\"\n        this is the structured output.\n    \"#)\n    context_delta string? @description(#\"\n        you have been provided with some context and some guidance. This is your interpretation of the consolidated context. In the\n        next iteration, this will be the changed context with no additional guidance IF this is different than the original context.\n    \"#)\n\n}\n\n\n\nfunction SampleInput(text: string) -> StepFrameIn {\n    client CustomGPT4oMini\n    prompt #\"\n        produce a multi step plan to execute on the ask.        \n\n        {{ctx.output_format}}\n\n        --- TEXT ---\n        {{text}}\n    \"#\n}\n\nfunction SampleOutput(inp_s: StepFrameIn) -> StepFrameOut {\n    client CustomGPT4oMini\n    prompt #\"\n        execute on all steps\n        {{ctx.output_format}}\n        --- ALL TASKS ---\n        {{inp_s.guidance}}\n    \"#\n}\n\n\n\n\n\nfunction JokeTeller(arg:string) -> string {\n    client CustomGPT4oMini    \n    prompt #\"\n\n        {{ctx.output_format}}\n\n        {{arg}}\n        \n    \"#\n}\n\n\nclass MonikerState {\n    topic string?          // generic focus of the task (formerly \"subject\"); optional for broad tasks\n    last_style string?     // last style/tone used (generic)\n    last_output string?    // last emitted human-facing text (generic)\n}\n\nclass MonikerGuidance {\n    topic string? @description(#\"\n        optional one-shot override for topic\n    \"#)         \n    style string?  @description(#\"\n        optional style/tone hint (e.g., \"dry\", \"formal\", \"playful\")\n    \"#)         \n}\n\nclass MonikerData {\n    topic string? @description(#\"\n     simple structured echo for consumers; extend as needed\n        \n    \"#)         \n}\n\nclass MonikerStepFrameIn {\n    step string \n    state MonikerState\n    guidance MonikerGuidance?\n}\n\nclass MonikerStepFrameOut {\n    step string\n    state MonikerState\n    next_step string\n    \n    text string @description(#\"\n        human-facing output for this step\n    \"#)           \n    data MonikerData @description(#\"\n        structured output (minimal, extendable)\n    \"#)      \n    done bool @description(#\"\n        true when the monikerâ€™s workflow is complete\n    \"#)             \n    notes string?  @description(#\"\n         short rationale/what changed\n    \"#)        \n}\n\n// Generic moniker runner: provide the task description + a StepFrame input.\n// Examples of task: \n//  - \"Tell a short witty joke about the topic.\"\n//  - \"Write a two-sentence summary of the topic.\"\n//  - \"List three key facts about the topic.\"\n\n\nfunction TellAJokeV2(in_arg:StepFrameIn)->StepFrameOut {\n    client CustomGPT4oMini\n    prompt #\"\n        Tell a joke about {{in_arg.context}}.\n\n        {{ctx.output_format}}\n\n        using following guidance :\n        {{in_arg.guidance}}\n\n    \"#\n}\n\nfunction TellAJoke(frame: MonikerStepFrameIn, task: string)-> MonikerStepFrameOut  {\n    client CustomGPT4oMini\n    prompt #\"\n         You are a moniker that performs exactly one step of work according to 'task'.\n\n        {{ctx.output_format}}\n\n        Requirements:\n        - Perform the task based on state and (if present) guidance.\n\n        - If guidance.topic is present, use it and update state.topic.\n        - If guidance.style is present, reflect it in the output and set state.last_style.\n\n        - Place the human-facing output in 'text'.\n\n        - Set data.topic to the final topic used for this step.\n        - Set state.last_output to the same 'text' value.\n        - Echo back the input 'step' as 'step'.\n        - set next_step to be the next step as indicated.\n\n        - Set 'done' true only if the task is inherently single-shot or indicates completion; otherwise false.\n\n        \n        # step 0:\n        - keep telling jokes\n        # step 1\n        - tell bigger jokes\n        - next_step 2.1\n        # step 2.1\n        - tell sad joke\n        - next_step 2.2\n        # step 2.2\n        - tell sad joke\n        - next_step 3\n        \n        # step 3\n        - ok we are done\n\n        -- TASK --\n        {{ task }}\n        -- END TASK --\n\n        -- INPUT FRAME --\n        {{ frame }}\n        -- END INPUT FRAME --\n    \"#\n}",
}

def get_baml_files():
    return _file_map